{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNwfMejC7QlRqkIb+ki0uvg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vannisurdi/repVanni/blob/main/EsercizioRecomSyst_Cesarini.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Esercizio Recommendation System Cesarini"
      ],
      "metadata": {
        "id": "SxH2zNGZ463S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "####\n",
        "# Importing libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# We'll also import seaborn, a Python graphing library\n",
        "import warnings # current version of seaborn generates a bunch of warnings that we'll ignore\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "from sklearn.svm import LinearSVC # Importing a classifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier,GradientBoostingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV # LIbrary for grid search\n",
        "\n",
        "\n",
        "# Library to compute time measures\n",
        "import time\n",
        "\n",
        "\n",
        "from sklearn.feature_selection import SelectPercentile\n",
        "from sklearn.feature_selection import f_classif,chi2\n",
        "from sklearn.preprocessing import Binarizer, scale\n",
        "\n",
        "\n",
        "from sklearn.metrics import roc_auc_score,roc_curve\n"
      ],
      "metadata": {
        "id": "iB3iMprO4394"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "####\n",
        "# Optional library\n",
        "# This code may benefit the seaborn library. If seaborn is not installed, then follow the next instructions\n",
        "# run on a code cell the following command\n",
        "# !pip install seaborn\n",
        "# If you fail in installing seaborn, don't worry, just skip the few sections where it is used\n",
        "import seaborn as sns\n"
      ],
      "metadata": {
        "id": "PH8AudAI5XGh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Link to download the dataset\n",
        "#url='https://www.dropbox.com/s/v7evl2h7svqqey0/train.csv?dl=1'\n",
        "url='https://drive.google.com/uc?export=download&id=1LqzdqmMwkElgkX_Wpr8PxMWD0xDWIffg'\n",
        "####\n",
        "# Loading data\n",
        "data1 = pd.read_csv(url, index_col='ID') #index_col is the column name containing the record numeric index\n",
        "#data2 = pd.read_csv('test.csv', index_col='ID') # error_bad_lines=False\n"
      ],
      "metadata": {
        "id": "cEoIOzRp5kWe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "####\n",
        "# Let's see what's in the trainings data\n",
        "# The .head() method print the first 5 records and the metadata (e.g., the csv column names)\n",
        "print('data1.shape')\n",
        "print(data1.shape)\n",
        "data1.head() # Last block element doesnt' need the print(), and ve we get also a nicer output\n"
      ],
      "metadata": {
        "id": "k773yzrm6D9J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "####\n",
        "print('Column names')\n",
        "colNames=data1.columns\n",
        "#colNames.sort() # let's sort them\n",
        "print(colNames)\n",
        "#print(type(colNames)) # <class 'pandas.core.indexes.base.Index'>\n",
        "print(\"Number of Columns\", len(colNames))\n",
        "#if you want to see them all, please uncomment below\n",
        "#print(colNames[0:100]) # From element of index 0 to element of index 99 (100 is the first out)\n",
        "#print(colNames[100:200])\n",
        "#print(colNames[200:300])\n",
        "#print(colNames[300:370])\n"
      ],
      "metadata": {
        "id": "uE2skqS36FpY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### usa il chi quadro e poi l'anova per capire quali variabili spiegano meglio/influenzano il target\n",
        "# li utilizza in maniera impropria, il risultato potrebbe essere NON statisticam. significativo\n",
        "# ok lo stesso perché per adesso mi serve solo selezionare le features più importanti!!!\n",
        "# sto partendo da 370 variabili, devo ridurre il numero prima di cominciare\n",
        "\n",
        "\n",
        "\n",
        "####\n",
        "# Feature Selection\n",
        "\n",
        "\n",
        "# Already imported\n",
        "# from sklearn.feature_selection import SelectPercentile\n",
        "# from sklearn.feature_selection import f_classif,chi2\n",
        "# from sklearn.preprocessing import Binarizer, scale\n",
        "\n",
        "\n",
        "def selectMostImportantFeatures(matrix,targetFeature, percentile=3):\n",
        "   # This function selects the top features based on chi2 and f_classif.\n",
        "   # The matrix parameter is a pandas dataframe containing the feature\n",
        "   # to be investigated.\n",
        "   # Two methods are used:\n",
        "   # * chi2. Chi2 selects the features\n",
        "   #   which are highly dependent on the response\n",
        "   #   Best Used for: Categorical which are Boolean,\n",
        "   #   Frequency and Counts that are non negative\n",
        "   # * f_classif (ANOVA F-value) for classification features. ANOVA can tell\n",
        "   #   if an independent variable (e.g. categorical) has significant influence\n",
        "   #   on Dependent (continuous or ordinal) variable\n",
        "   # * The percentile parameter is the treshold for selecting the more relevant features \n",
        "   # \n",
        "   # for the targetFeature classification\n",
        "   # More info on http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.chi2.html\n",
        "\n",
        "\n",
        "   matrix_bin = Binarizer().fit_transform(scale(matrix)) # standardize and binarize\n",
        "   selectChi2 = SelectPercentile(chi2, percentile=percentile).fit(matrix_bin, targetFeature)\n",
        "   selectF_classif = SelectPercentile(f_classif, percentile=percentile).fit(matrix, targetFeature)\n",
        "\n",
        "\n",
        "   chi2_selected = selectChi2.get_support() # boolean values # numpy.ndarray dtype=bool # Get a mask of the features selected\n",
        "   chi2_selected_features = [ f for i,f in enumerate(matrix.columns) if chi2_selected[i]]\n",
        "   #print('Chi2 selected {} features {}.'.format(chi2_selected.sum(), chi2_selected_features))\n",
        "\n",
        "\n",
        "   f_classif_selected = selectF_classif.get_support()\n",
        "   f_classif_selected_features = [ f for i,f in enumerate(matrix.columns) if f_classif_selected[i]]\n",
        "   #print('F_classif selected {} features {}.'.format(f_classif_selected.sum(), f_classif_selected_features))\n",
        "  \n",
        "   commonSelected = chi2_selected & f_classif_selected # computing a selection mask focusing on the intersection of the two feature sets\n",
        "   #print('Chi2 & F_classif selected {} features'.format(selected.sum()))\n",
        "  \n",
        "   commonFeatures = [ f for f,s in zip(matrix.columns, commonSelected) if s]\n",
        "   return commonFeatures\n",
        "\n"
      ],
      "metadata": {
        "id": "O0_To0gk6HXd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "####\n",
        "target = data1['TARGET']\n",
        "bestFeature_V1=selectMostImportantFeatures(data1, target)\n",
        "print(bestFeature_V1)\n",
        "# TARGET is in the results, this is ok\n",
        "\n",
        "\n",
        "###scoprirò che il dataset è sbilanciato\n"
      ],
      "metadata": {
        "id": "ksRCqpJD7Ube"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "####\n",
        "# Happy customers have TARGET==0, unhappy custormers have TARGET==1\n",
        "# From now on: Happy0, Unhappy1\n",
        "print('Value_counts() result for targetColumn')\n",
        "print('Remember: Happy0, Unhappy1')\n",
        "print('Label Count')\n",
        "print(target.value_counts())\n",
        "\n",
        "\n",
        "#Let's create a prettier synthetic dataframe (i.e., a new data structure)\n",
        "df = pd.DataFrame(target.value_counts())\n",
        "print(\"Let's rename the TARGET column name to TARGETNUM\")\n",
        "df=df.rename(columns={'TARGET':'TARGETNUM'})\n",
        "# Adding the Percentage column\n",
        "recordNum= len(target)\n",
        "df['Percentage'] = 100.0*df['TARGETNUM']/recordNum\n",
        "df #The last command of the block is printed even if there is no print()\n",
        "# Comments are not instructions\n",
        "# A little less then 4% are unhappy => unbalanced dataset\n",
        "\n"
      ],
      "metadata": {
        "id": "DzZBNSq37XXl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### 2 versioni alternative, sotto fanno la stessa cosa\n",
        "\n",
        "####\n",
        "# Identifying constant columns\n",
        "constColNames = []\n",
        "for col in data1.columns: # Iterating over column names\n",
        "   if data1[col].std() == 0: # .std() computes the standard deviation over all column values. If .std()== 0 it means the column values are all equals\n",
        "       constColNames.append(col)\n",
        "print(constColNames)\n",
        "print('Total Number of Features', len(data1.columns),'Number of Constant Features', len(constColNames))\n",
        "\n",
        "\n",
        "\n",
        "####\n",
        "# A different way to identify constant columns\n",
        "# Using python list comprehension\n",
        "constColNames2 = [col for col in data1.columns if data1[col].std() == 0]\n",
        "print(constColNames2)\n",
        "print('Total Number of Features', len(data1.columns),'Number of Constant Features', len(constColNames2))"
      ],
      "metadata": {
        "id": "TZuzznas7oJ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Removing constant Columns\n",
        "data1.drop(constColNames, axis=1, inplace=True) # inplace=True: work inplace, do not create a new (reduced) data structure\n",
        "# axis=1, 0 is rows, 1 is columns, axis=1 meas: matching column names\n"
      ],
      "metadata": {
        "id": "_T-XMML28kp7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "####\n",
        "# Identify duplicated columns\n",
        "# i.e., columns that have exactly the same values\n",
        "duplicateColumns = []\n",
        "colNames = data1.columns\n",
        "for i in range(len(colNames)-1): # i ranges from 0 to colNames_length -1\n",
        "   v = data1[colNames[i]].values # v contains the i_th column values\n",
        "   for j in range(i+1,len(colNames)):\n",
        "       # Checking if the column j was not already identified as duplicate\n",
        "       # and if column j has the same content as column v\n",
        "       if colNames[j] not in duplicateColumns and np.array_equal(v,data1[colNames[j]].values):\n",
        "           duplicateColumns.append(colNames[j])\n",
        "print(duplicateColumns)\n",
        "print(len(duplicateColumns))\n"
      ],
      "metadata": {
        "id": "RQ4Q29tk8_mr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "####\n",
        "# Removing duplicate columns\n",
        "data1.drop(duplicateColumns, axis=1, inplace=True) # inplace=True: work inplace, do not create a new (reduced) data structure\n",
        "# axis=1, 0 is rows, 1 is columns, axis=1 meas: matching column names\n"
      ],
      "metadata": {
        "id": "K4xkxwwa9ZG9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "####\n",
        "# var3 is suspected to be the nationality of the customer\n",
        "# Don't share this news. It's confidential!!!\n",
        "# Computing a descriptive statistics of var3\n",
        "vc=data1['var3'].value_counts()\n",
        "print('var3 values      frequencies')\n",
        "print(vc)\n",
        "print()\n",
        "print(\"They are sorted from the most to the less frequent one.\")\n",
        "print(\"Let's focus on the top 10 most frequent\")\n",
        "print(vc[:10]) #[0:10] or shortly [:10] extracts the first 10 rows\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "7UEYQxYc9dPE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "####\n",
        "# var3 is suspected to be the nationality of the customer\n",
        "# Don't share this news. It's confidential!!!\n",
        "# Computing a descriptive statistics of var3\n",
        "vc=data1['var3'].value_counts()\n",
        "print('var3 values      frequencies')\n",
        "print(vc)\n",
        "print()\n",
        "print(\"They are sorted from the most to the less frequent one.\")\n",
        "print(\"Let's focus on the top 10 most frequent\")\n",
        "print(vc[:10]) #[0:10] or shortly [:10] extracts the first 10 rows\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "####\n",
        "# -999999 it is very likely a null value i.e., the nationality is unknown\n",
        "# Let's replace this null with the most common value\n",
        "mask=data1['var3']==-999999 #\n",
        "\n",
        "\n",
        "# What is mask?\n",
        "print('mask.shape', mask.shape)\n",
        "print(\"data1['var3'].shape\", data1['var3'].shape)\n",
        "\n",
        "\n",
        "print('mask[780:790]')\n",
        "print(mask[780:790])\n",
        "\n",
        "\n",
        "print(data1['var3'][780:790])\n",
        "\n",
        "\n",
        "print(\"Mask cells are true when the corresponding cells in X1['var3'] are -999999\")\n",
        "\n",
        "\n",
        "# Now using mask to replace -999999 with 2\n",
        "data1['var3'][mask]=2\n",
        "print('')\n",
        "print(\"Let's have a look to the modified var3 column\")\n",
        "print(data1['var3'][780:790])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "####\n",
        "# Add feature that counts the number of zeros in a row\n",
        "\n",
        "\n",
        "# I need to exclude TARGET, which is full of zeroes\n",
        "data1NoTarget = data1.drop(['TARGET'], axis=1)\n",
        "# data1NoTarget might have been computed in the following way\n",
        "# data1NoTarget = train.iloc[:,:-1] # target is the last\n",
        "\n",
        "\n",
        "# The next command performs elementwise the test ...==0\n",
        "# Then the True values are summed  row-wise (i.e., along axis 1)\n",
        "# (True is also interpreted as 1)\n",
        "data1NoTarget['n0'] = (data1NoTarget==0).sum(axis=1)\n",
        "data1['n0'] = data1NoTarget['n0']\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "TOWoY6OQ94WK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Krgz6hqf_fJT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}